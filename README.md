# final_project_path_finding

## Project Description
The goal of this project was pared down over time as it became clear that the scope was too large. Ultimately, the goal was to build a pathfinding robot that would use the A* algorithm to find it's way around the map. This also relied on precise localisation to identify the robot's location and then plan the resulting path. It was interesting to explore an entirely different paradigm of movement than we have explored in class so far. In every other project we have relied on constantly updating lidar scans or image data in order to direct the robot. It never had a goal or destination that it could not see. In this project I had to figure out how to actually transform a discrete "path" of coordinate tuples to an actual path that the robot could follow in the world. This was the majority of the work in the project and I went through many iterations getting it to work. But, it was satisfying getting at least some functionality when other groups couldn't. To summarise the action of the robot: On startup the robot would move until it was able to reliably generate a converged localisation estimate. Given this position, it would then generate a path using the A* algorithm to travel to a specified point within the map, avoiding any obstacles. This was then converted to real coordinates and overlaid on the localisation map. Finally the robot would travel along this path to the destination. 

## System Architecture
As above, the three main components are localisation through the likelihood field algorithm, path-planning through A*, and path-following through my own attempted algorithms. 

Going in order, the localisation was much the same as the particle filter project. However, I made a couple changes to my pre-existing work to optimise that algorithm. For example, Instead of an average sum of resampled particles, I found that using a weighted average when calculating robot_estimate position worked better. I also had to spend a lot of time to tune all the relevant parameters, restrict the model to counteract the presence of the arm, add new class variables etc.  (this is not weighting within resampling. It is in addition to that.) All of this module is within particle_filter.py. In order for the robot to have an estimated position for it's path planning, in the initialisation of the main class, the robot slowly moves forward until it receives a new estimated position a set number of times. I found that 4 iterations was the perfect amount for (more) reliable convergence, without adding unneccessary movement and time. This does present a small restriction on where the robot can be started. 

The second large component was generating a path. This process is called through the aStar() class method. This then calls convert_map_to_graph, which is basically another instance of converting occupancy grid values into cell coordinates, just in this file rather than particle_filter.py. The critical next step is to modify this map for the reality of the Turtlebot. The Turtlebot has a non-trivial width and in order for it to move around obstacles, it cannot just pass by in the adjacent map cell. So, in occupied_in_radius(), we transform such that any cell with an occupied cell within a radius, is also classified as a an occupied cell. Therefore, the A* algorithm can then generate realistic paths for the Turtlebot. After that, we run the canonical A* algorithm within fill_path. But in order for this to generate an actual path, more work is done within aStar() and movement functions to generate a linear path from the map of g and h scores. The other interesting part of aStar() is that I wrote a section that then transforms this path into a PoseArray() so it can be displayed on the Rviz map. 

Now comes the hard part. Move_to_dest sets the current localised position estimate as the start point of the path, runs aStar() to create a path and then iterates through the path by executing move_to_next_point. This is the most important function within the program and is what allows the robot to follow the path given to it. I made detailed and lengthy attempts at using both active localisation and odometry for the robot to follow its path. I have included versions of those methods as comment blocks within the existing function to record that work. After trying those methods for many hours, the only way that worked was the crudest. Instead of constantly readjusting it's estimated position, the robot simply takes in the whole path and deterministically iterates through it by calculcating the angle between path points, adding or substracting a rotation based on that fixed data for every path point, and then moving forward a fixed distance for each point. This method only relies on the one position estimate calculated before the robot begins following the path. Initially I had the robot try to constantly estimate it's position through localisation and actively rotate and drive to each point in order. However, the localisation was never accurate enough over long periods of time so that the robot could move moothly along the path. If the yaw or position changes momentarily the robot would go off path and defeat the purpose of the program. It was also impossible to create accurate rotations along the path because the localisation could not update with enough frequency to create precise turns. The evidence of this is visible within the large commented block. I then tried to use odometry in order to measure precise turns. This also did not work because the odometry is tied to the localisation estimate and would not keep an accurate estimate over many path points. Finally, I settled on only using the first localisation position estimate. After this estimate, the robot calculates the angle to the first path point, rotates through that angle and drives a fixed distance to it.  Once this is done, it simply assumes that it is at the first point in the path. It calculates the angle difference between the previously saved yaw and the line between the next path point and it. It rotates through this differential and drives forward both at fixed speeds.  There was a lot of fine-tuning here to compensate for lag, the cardboard, speed of rotation, movement, acceleration etc. At one point even the atan() function appeared to work incorrectly and so I coded up a manual angle calculator, realising that there were only ever 8 directions in the grid. Yet without active updating, I achieved a relatively accurate path follower. Once it iterates through every point in the path, the robot stops moving. I was not able to include user-inputted destinations given the constraints we have discussed, but you can change self.end in the file. 

## ROS Node Diagram
<img width="841" alt="Screen Shot 2022-12-09 at 4 39 47 am" src="https://user-images.githubusercontent.com/115107885/206683984-7f7fabc4-dc8b-4765-bcad-a29091c455e0.png">


## Execution
You have to run the map_launch launch file first. This may need to be edited to use the correct home directory for your machine. Secondly, run path_finder.py
I ran "roslaunch final_project_path_finding map_launch.launch" and "rosrun final_project_path_finding path_finder.py"

## Challenges, Future Work and Takeaways

There were many many challenges in this project. Even from the idea stage, it was hard to come up with something that was actually within scope for this project. I had to change and edit ideas multiple times. Even after that, the scope had to be reduced while the project was ongoing. One overcomes this with flexibility and constant creativity. The biggest challenge was definitely path following - converting discrete, mapped points into a movement path for the robot operating within continuous dimensions. We never actually used our localisation software to achieve any movement goals in this class and this is clear to me now. It is incredibly difficult to achieve consistent, frequent, and precise localisation estimates. Even odometry cannot be trusted to achieve reliable results. But, as with everything else, you can overcome these challenges by coming up with alternatives that circumvent fundamental issues. Another huge issue was teamwork. 

In the future, the most obvious development would be to change the path following method. This would require a complete overhaul of the localisation software to create a usable product. In addition to creating a reactive path follower through continuous localisation, ideally the robot should also incorporate lidar scanning to avoid variable obstacles and react to any changes within the environment. For example, if the robot is following the path and scans an unexpected obstacle within its lidar range, it would add this to the obstacle map and generate a new path for it to follow to its destination. There are also a lot of other fun additions to this project that we discussed in our initial writeup. Once path-following works flawlessly, the robot should then be able to pick up and deliver objects. In the future, the program should also support user input for destinations. 

The biggest takeaway regards planning in robotics. As I discussed at the top, none of the previous projects required "planning" of any sort. Rather, the robot just reacts to its environment and follows whatever conditions it's set to look for. Planning and executing actions that do not rely on external stimuli - and keeping those actions on track - is an entirely different beast. This leads into the second point, which is a more general discussion of theory and practice in robotics. Ultimately the most theoretically complex aspect of this, the A* algorithm, was the easiest and simplest part to actually implement. Instead the real challenge in this style of robotics is implementing whatever fancy theoretical optimisation one might come up with. Having a robust platform for practical work is critical to making progress within robotics, without it you will spend 95% of the time debugging. 

# DEMO VIDEO 
NB: you can watch a natural speed, less compressed, larger video in the github repository. This was the best quality I could get into the readme. 

The video shows the robot automatically moving to generate a converged and accurate localisation estimate. Once it has that, it generates a path to its destination using the A* algorithm which is shown on the Rviz map and as a numerical cell-coordinate path within the terminal output. It then executes that path as described above. Near the end of the video I put back a brick that was missing in order to more accurately show the obstacles that the robot is avoiding on its path. 


https://user-images.githubusercontent.com/115107885/206743200-cafe2709-1e7a-4a4a-83bb-7ca5bba2ee95.mp4


